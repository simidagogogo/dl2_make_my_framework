from step02 import Variable, Function
from step03 import Square, Exp
import numpy as np

# 实现Variable类和Function类，目的是实现自动微分
# 两种求倒数方法：1)数值微分, 2)反向传播
# 1)数值微分: 利用微小的差值获得函数变化量的方法
# 2)反向传播: 
# 3) 导数也可以通过解析解的方式求解。解析解的方式求解是指只通过式子的变形推导出答案

# 什么是导数? 简单地说，导数是变化率，它被定义为在极短时间内的变化量
# 位置的导数：某个物体的位置相对于时间的变化率，即速度
# 速度的导数：连度相对于时间的变化率，即加速度

# 中心差分近似
# 数值微分使用非常小的值h求出的是真的导数的近似值, 因此这个值包含误差
# 中心差分近似是减小近似误差的一种方法. 中心差分近似计算的不是f(x)和f(x+h)的差, 而是f(x-h)和 f(x+h)的差
# 计算过x和x+h这两点的直线的斜率的方法称为前向差分近似; 计算x-h和x+h这两点斜率的方法称为中心差分近似, 中心差分近似实际产生的误差更小

# 数值微分存在的问题
# 1)数值微分的结果包含误差. 在多数情况下, 这个误差非常小, 但在一些情况下, 计算产生的误差可能会很大
# 数值微分的结果中容易包含误差的主要原因是“精度丢失”。
# 中心差分近似等求差值的方法计算的是相同量级数值之间的差，
# 但由于精度丢失，计算结果中会出现有效位数减少的情况。以有效位数为4的情况为例，
# 在计算两个相近的值之间的差时，比如1.234-1.233，其结果为0.001，有效位数只有1位。
# 本来可能是1.234...-1.233...=0.001434...之类的结果，但由于精度丢失，结果变成0.001。
# 同样的情况也会发生在数值微分的差值计算中，精度丢失使结果更容易包含误差。

# 2)数值微分更严重的问题是计算成本高。
# 具体来说，在求多个变量的导数时，程序需要计算每个变量的导数。有些神经网络包含几百万个以上的变量（参数），
# 通过数值微分对这么多的变量求导是不现实的。这时，反向传播就派上了用场。

# 另外，数值微分可以轻松实现，并能计算出大体正确的数值。
# 而反向传播是一种复杂的算法，实现时容易出现bug。
# 我们可以使用数值微分的结果检查反向传播的实现是否正确。
# 这种做法叫作梯度检验（gradient checking），它是一种将数值微分的结果与反向传播的结果进行比较的方法。
# 步骤10实现了梯度检验。

def numerical_diff(f: Function, x: Variable, eps: float = 1e-4):
    x0 = Variable(x.data - eps)
    x1 = Variable(x.data + eps)
    y0 = f(x0)
    y1 = f(x1)
    return (y1.data - y0.data) / (2 * eps)

def f2(x: Variable):
    A = Square()
    B = Exp()
    C = Square()
    return C(B(A(x)))

if __name__ == "__main__":
    f = Square()
    x = Variable(np.array(2.0))
    dy = numerical_diff(f, x)
    # print(dy)

    x = Variable(np.array(0.5))
    dy = numerical_diff(f2, x)
    print(f"dy: ", dy)
